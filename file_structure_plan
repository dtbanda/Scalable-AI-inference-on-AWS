llm-aws-deployment/
â”‚
â”œâ”€â”€ api/                          # FastAPI app code
â”‚   â”œâ”€â”€ main.py                   # Entry point: /generate route
â”‚   â”œâ”€â”€ model_loader.py           # Loads the Liquid AI SLM model
â”‚   â”œâ”€â”€ inference.py              # Handles prompt â†’ response logic
â”‚   â””â”€â”€ utils.py                  # Any helper functions
â”‚
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile                # Docker setup for running the API
â”‚
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ main.tf                   # Terraform: EC2, IAM, Security Group
â”‚   â”œâ”€â”€ variables.tf              # TF variables
â”‚   â”œâ”€â”€ outputs.tf                # TF outputs (public IP, etc.)
â”‚   â””â”€â”€ terraform.tfvars.example # Example values
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ push_to_ecr.sh           # Script to push Docker image to AWS ECR
â”‚   â””â”€â”€ deploy_model.sh          # Script to run API on EC2 after SSH
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ prompt_sample.json        # Example API request
â”‚   â””â”€â”€ response_sample.json      # Example API response
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ deploy.yml           # (Optional) GitHub Actions CI/CD
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md                    # ðŸ“„ Your detailed project documentation
â””â”€â”€ architecture.png             # ðŸ“Š Diagram image used in README
